# Neural Network from Scratch

A neural network framework implemented in Python, supporting various activation functions, loss functions, and optimizers for classification and regression tasks.

## ğŸš€ Features

- Custom Dense layers with configurable activation types (ReLU, tanh, sigmoid, softmax)
- Multiple loss functions (MSE, Binary Crossentropy, Categorical Crossentropy)
- Gradient descent optimizer with learning rate decay and momentum
- Modular design for easy extension and experimentation

## ğŸ›  Tech Stack

- Python
- NumPy

## ğŸ“ Project Structure

project/
â”œâ”€â”€ utils.py # Neural network classes (Dense, activations, losses, optimizer) <br>
â”œâ”€â”€ main.py # Training script and model setup <br>
â”œâ”€â”€ plots/ # plots saved <br>
â””â”€â”€ README.md # Project documentation <br>
<br>
The tanh_implementation.py file is still in progress and testing systems will be completed in further updates

main.py is a fully functional file and demonstrates the application of the Softmax activation with Adam Optimizer as the optimization.
